# TechLeadSWEA - Technical Leadership Agent
# Purpose: Coordinate system generation and manage quality gates

## EXAMPLES

### System Coordination Example
```python
from typing import Dict, Any, List, Optional
import logging
from datetime import datetime

logger = logging.getLogger(__name__)

class <EntityName>SystemCoordinator:
    """Technical coordinator for <EntityName> system generation"""

    def __init__(self):
        self.architecture_decisions = {}
        self.quality_standards = {
            "code_quality": 0.8,
            "test_coverage": 0.9,
            "performance_threshold": 2.0,  # seconds
            "security_score": 0.85
        }
        self.coordination_history = []

    def coordinate_system_generation(self, entity_attributes: List[str], context: str) -> Dict[str, Any]:
        """Coordinate the generation of <EntityName> system components"""
        logger.info(f"üéØ Starting <EntityName> system coordination for attributes: {entity_attributes}")

        # Phase 1: Architecture Planning
        architecture_plan = self._plan_architecture(entity_attributes, context)

        # Phase 2: Component Coordination
        coordination_plan = self._create_coordination_plan(architecture_plan)

        # Phase 3: Quality Standards Definition
        quality_gates = self._define_quality_gates(architecture_plan)

        # Phase 4: Risk Assessment
        risk_assessment = self._assess_risks(architecture_plan)

        coordination_result = {
            "architecture_plan": architecture_plan,
            "coordination_plan": coordination_plan,
            "quality_gates": quality_gates,
            "risk_assessment": risk_assessment,
            "coordination_timestamp": datetime.now().isoformat(),
            "entity_type": "<EntityName>",
            "context": context
        }

        self.coordination_history.append(coordination_result)
        logger.info(f"‚úÖ <EntityName> system coordination completed successfully")

        return coordination_result

    def _plan_architecture(self, attributes: List[str], context: str) -> Dict[str, Any]:
        """Plan the technical architecture for <EntityName> system"""
        architecture = {
            "database_schema": {
                "table_name": "<entity_name_lower>s",
                "primary_key": "id",
                "indexes": ["email", "name"],
                "constraints": ["unique_email", "not_null_name"],
                "relationships": []
            },
            "api_design": {
                "base_path": "/api/<entity_name_lower>s",
                "endpoints": ["GET", "POST", "PUT", "DELETE"],
                "response_format": "JSON",
                "authentication": "None",  # For PoC
                "rate_limiting": "Disabled"  # For PoC
            },
            "ui_components": {
                "main_page": "<EntityName> Management System",
                "forms": ["create_<entity_name_lower>", "edit_<entity_name_lower>"],
                "tables": ["<entity_name_lower>_list"],
                "navigation": ["Dashboard", "CRUD Operations"]
            },
            "testing_strategy": {
                "unit_tests": "Pytest with mocking",
                "integration_tests": "API endpoint testing",
                "ui_tests": "Streamlit component testing",
                "coverage_target": 90
            }
        }

        # Add context-specific modifications
        if context == "academic":
            architecture["database_schema"]["relationships"].append("<related_entity_lower>s")
            architecture["ui_components"]["forms"].append("enrollment_form")

        return architecture

    def _create_coordination_plan(self, architecture: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Create detailed coordination plan for SWEA agents"""
        coordination_plan = [
            {
                "phase": "Database Setup",
                "swea_agent": "DatabaseSWEA",
                "task": "setup_database",
                "priority": "HIGH",
                "dependencies": [],
                "expected_output": "SQLAlchemy model and database initialization",
                "quality_criteria": ["proper_schema", "indexes_created", "constraints_enforced"]
            },
            {
                "phase": "Backend Development",
                "swea_agent": "BackendSWEA",
                "task": "generate_model",
                "priority": "HIGH",
                "dependencies": ["DatabaseSWEA.setup_database"],
                "expected_output": "Pydantic models with validation",
                "quality_criteria": ["type_safety", "validation_rules", "business_logic"]
            },
            {
                "phase": "Backend Development",
                "swea_agent": "BackendSWEA",
                "task": "generate_api",
                "priority": "HIGH",
                "dependencies": ["BackendSWEA.generate_model"],
                "expected_output": "FastAPI endpoints with CRUD operations",
                "quality_criteria": ["restful_design", "error_handling", "status_codes"]
            },
            {
                "phase": "Frontend Development",
                "swea_agent": "FrontendSWEA",
                "task": "generate_ui",
                "priority": "MEDIUM",
                "dependencies": ["BackendSWEA.generate_api"],
                "expected_output": "Streamlit UI with forms and tables",
                "quality_criteria": ["user_experience", "responsive_design", "error_display"]
            },
            {
                "phase": "Testing",
                "swea_agent": "TestSWEA",
                "task": "generate_all_tests_with_collaboration",
                "priority": "HIGH",
                "dependencies": ["FrontendSWEA.generate_ui"],
                "expected_output": "Comprehensive test suite",
                "quality_criteria": ["test_coverage", "edge_cases", "performance_tests"]
            },
            {
                "phase": "Quality Assurance",
                "swea_agent": "TechLeadSWEA",
                "task": "review_and_approve",
                "priority": "CRITICAL",
                "dependencies": ["TestSWEA.generate_all_tests_with_collaboration"],
                "expected_output": "System approval and deployment readiness",
                "quality_criteria": ["overall_quality", "integration_score", "deployment_ready"]
            }
        ]

        return coordination_plan

    def _define_quality_gates(self, architecture: Dict[str, Any]) -> Dict[str, Any]:
        """Define quality gates for <EntityName> system"""
        quality_gates = {
            "code_quality": {
                "metric": "Code Quality Score",
                "threshold": 0.8,
                "measurement": "Static analysis and code review",
                "failure_action": "Require code improvements"
            },
            "test_coverage": {
                "metric": "Test Coverage Percentage",
                "threshold": 90,
                "measurement": "Pytest coverage report",
                "failure_action": "Require additional tests"
            },
            "api_performance": {
                "metric": "API Response Time",
                "threshold": 2.0,  # seconds
                "measurement": "Load testing",
                "failure_action": "Performance optimization required"
            },
            "data_integrity": {
                "metric": "Data Validation Success Rate",
                "threshold": 100,
                "measurement": "Validation test suite",
                "failure_action": "Fix validation logic"
            },
            "ui_usability": {
                "metric": "UI Component Functionality",
                "threshold": 100,
                "measurement": "UI test automation",
                "failure_action": "UI improvements required"
            }
        }

        return quality_gates

    def _assess_risks(self, architecture: Dict[str, Any]) -> Dict[str, Any]:
        """Assess technical risks for <EntityName> system"""
        risks = {
            "high_risk": [
                {
                    "risk": "Database schema conflicts",
                    "probability": "MEDIUM",
                    "impact": "HIGH",
                    "mitigation": "Comprehensive testing and migration planning"
                },
                {
                    "risk": "API endpoint failures",
                    "probability": "LOW",
                    "impact": "HIGH",
                    "mitigation": "Robust error handling and monitoring"
                }
            ],
            "medium_risk": [
                {
                    "risk": "UI performance issues",
                    "probability": "MEDIUM",
                    "impact": "MEDIUM",
                    "mitigation": "Performance testing and optimization"
                },
                {
                    "risk": "Test coverage gaps",
                    "probability": "MEDIUM",
                    "impact": "MEDIUM",
                    "mitigation": "Comprehensive test planning and execution"
                }
            ],
            "low_risk": [
                {
                    "risk": "Code style inconsistencies",
                    "probability": "HIGH",
                    "impact": "LOW",
                    "mitigation": "Code formatting and linting tools"
                }
            ]
        }

        return risks
```

### Quality Gate Management Example
```python
from typing import Dict, Any, List, Optional
import logging
from datetime import datetime

logger = logging.getLogger(__name__)

class <EntityName>QualityManager:
    """Quality gate manager for <EntityName> system"""

    def __init__(self):
        self.quality_metrics = {}
        self.gate_results = {}
        self.approval_history = []

    def review_and_approve(self, execution_results: List[Dict[str, Any]], context: str) -> Dict[str, Any]:
        """Review system components and approve for deployment"""
        logger.info(f"üîç Starting <EntityName> system review and approval")

        # Phase 1: Component Quality Assessment
        component_scores = self._assess_component_quality(execution_results)

        # Phase 2: Integration Testing
        integration_score = self._assess_integration(execution_results)

        # Phase 3: Performance Evaluation
        performance_score = self._assess_performance(execution_results)

        # Phase 4: Security Review
        security_score = self._assess_security(execution_results)

        # Phase 5: Overall Approval Decision
        overall_approval = self._make_approval_decision(
            component_scores, integration_score, performance_score, security_score
        )

        approval_result = {
            "overall_approval": overall_approval,
            "deployment_ready": overall_approval,
            "system_quality_score": self._calculate_overall_score(
                component_scores, integration_score, performance_score, security_score
            ),
            "component_scores": component_scores,
            "integration_score": integration_score,
            "performance_score": performance_score,
            "security_score": security_score,
            "approval_timestamp": datetime.now().isoformat(),
            "entity_type": "<EntityName>",
            "context": context,
            "quality_gates_passed": self._count_passed_gates(component_scores),
            "total_quality_gates": len(component_scores)
        }

        self.approval_history.append(approval_result)
        self.gate_results = approval_result

        if overall_approval:
            logger.info(f"‚úÖ <EntityName> system approved for deployment")
        else:
            logger.warning(f"‚ùå <EntityName> system requires improvements before deployment")

        return approval_result

    def _assess_component_quality(self, execution_results: List[Dict[str, Any]]) -> Dict[str, float]:
        """Assess quality of individual system components"""
        component_scores = {}

        for result in execution_results:
            task_name = result.get("task", "unknown")
            success = result.get("success", False)

            if success:
                # Assess quality based on task type
                if "DatabaseSWEA" in task_name:
                    component_scores["database_quality"] = self._assess_database_quality(result)
                elif "BackendSWEA" in task_name and "model" in task_name:
                    component_scores["model_quality"] = self._assess_model_quality(result)
                elif "BackendSWEA" in task_name and "api" in task_name:
                    component_scores["api_quality"] = self._assess_api_quality(result)
                elif "FrontendSWEA" in task_name:
                    component_scores["ui_quality"] = self._assess_ui_quality(result)
                elif "TestSWEA" in task_name:
                    component_scores["test_quality"] = self._assess_test_quality(result)
            else:
                # Failed tasks get low scores
                if "DatabaseSWEA" in task_name:
                    component_scores["database_quality"] = 0.0
                elif "BackendSWEA" in task_name:
                    component_scores["backend_quality"] = 0.0
                elif "FrontendSWEA" in task_name:
                    component_scores["ui_quality"] = 0.0
                elif "TestSWEA" in task_name:
                    component_scores["test_quality"] = 0.0

        return component_scores

    def _assess_database_quality(self, result: Dict[str, Any]) -> float:
        """Assess database component quality"""
        quality_score = 0.8  # Base score

        # Check for proper schema creation
        if result.get("result", {}).get("tables_created"):
            quality_score += 0.1

        # Check for proper constraints
        if result.get("result", {}).get("constraints_enforced"):
            quality_score += 0.1

        return min(quality_score, 1.0)

    def _assess_model_quality(self, result: Dict[str, Any]) -> float:
        """Assess Pydantic model quality"""
        quality_score = 0.8  # Base score

        # Check for proper validation
        if result.get("result", {}).get("validation_rules"):
            quality_score += 0.1

        # Check for type safety
        if result.get("result", {}).get("type_safety"):
            quality_score += 0.1

        return min(quality_score, 1.0)

    def _assess_api_quality(self, result: Dict[str, Any]) -> float:
        """Assess API endpoint quality"""
        quality_score = 0.8  # Base score

        # Check for proper CRUD operations
        if result.get("result", {}).get("endpoints_created"):
            quality_score += 0.1

        # Check for error handling
        if result.get("result", {}).get("error_handling"):
            quality_score += 0.1

        return min(quality_score, 1.0)

    def _assess_ui_quality(self, result: Dict[str, Any]) -> float:
        """Assess UI component quality"""
        quality_score = 0.8  # Base score

        # Check for proper forms
        if result.get("result", {}).get("ui_components"):
            quality_score += 0.1

        # Check for user experience
        if result.get("result", {}).get("user_experience"):
            quality_score += 0.1

        return min(quality_score, 1.0)

    def _assess_test_quality(self, result: Dict[str, Any]) -> float:
        """Assess test suite quality"""
        quality_score = 0.8  # Base score

        # Check for test coverage
        if result.get("result", {}).get("test_coverage", 0) > 80:
            quality_score += 0.1

        # Check for comprehensive tests
        if result.get("result", {}).get("comprehensive_tests"):
            quality_score += 0.1

        return min(quality_score, 1.0)

    def _assess_integration(self, execution_results: List[Dict[str, Any]]) -> float:
        """Assess system integration quality"""
        successful_components = sum(1 for result in execution_results if result.get("success", False))
        total_components = len(execution_results)

        if total_components == 0:
            return 0.0

        integration_score = successful_components / total_components

        # Bonus for all components working together
        if integration_score == 1.0:
            integration_score = min(integration_score + 0.1, 1.0)

        return integration_score

    def _assess_performance(self, execution_results: List[Dict[str, Any]]) -> float:
        """Assess system performance"""
        # For PoC, assume good performance if all components are successful
        successful_components = sum(1 for result in execution_results if result.get("success", False))
        total_components = len(execution_results)

        if total_components == 0:
            return 0.0

        performance_score = successful_components / total_components

        return performance_score

    def _assess_security(self, execution_results: List[Dict[str, Any]]) -> float:
        """Assess system security"""
        # For PoC, assume basic security if all components are successful
        successful_components = sum(1 for result in execution_results if result.get("success", False))
        total_components = len(execution_results)

        if total_components == 0:
            return 0.0

        security_score = successful_components / total_components

        return security_score

    def _make_approval_decision(self, component_scores: Dict[str, float],
                              integration_score: float, performance_score: float,
                              security_score: float) -> bool:
        """Make final approval decision"""
        # Calculate overall score
        overall_score = self._calculate_overall_score(
            component_scores, integration_score, performance_score, security_score
        )

        # Approval criteria
        min_overall_score = 0.7
        min_integration_score = 0.8
        min_component_scores = 0.6

        # Check overall score
        if overall_score < min_overall_score:
            return False

        # Check integration score
        if integration_score < min_integration_score:
            return False

        # Check component scores
        for component, score in component_scores.items():
            if score < min_component_scores:
                return False

        return True

    def _calculate_overall_score(self, component_scores: Dict[str, float],
                               integration_score: float, performance_score: float,
                               security_score: float) -> float:
        """Calculate overall system quality score"""
        if not component_scores:
            return 0.0

        # Weighted average of all scores
        component_avg = sum(component_scores.values()) / len(component_scores)

        overall_score = (
            component_avg * 0.4 +
            integration_score * 0.3 +
            performance_score * 0.2 +
            security_score * 0.1
        )

        return overall_score

    def _count_passed_gates(self, component_scores: Dict[str, float]) -> int:
        """Count number of quality gates passed"""
        passed_gates = 0

        for component, score in component_scores.items():
            if score >= 0.7:  # Minimum passing score
                passed_gates += 1

        return passed_gates
```

### Conflict Resolution Example
```python
from typing import Dict, Any, List, Optional
import logging
from datetime import datetime

logger = logging.getLogger(__name__)

class <EntityName>ConflictResolver:
    """Conflict resolver for <EntityName> system coordination"""

    def __init__(self):
        self.conflict_history = []
        self.resolution_strategies = {
            "dependency_conflict": self._resolve_dependency_conflict,
            "naming_conflict": self._resolve_naming_conflict,
            "architecture_conflict": self._resolve_architecture_conflict,
            "quality_conflict": self._resolve_quality_conflict
        }

    def resolve_technical_conflict(self, conflict_data: Dict[str, Any]) -> Dict[str, Any]:
        """Resolve technical conflicts between SWEA agents"""
        logger.info(f"üîß Resolving technical conflict: {conflict_data.get('conflict_type', 'unknown')}")

        conflict_type = conflict_data.get("conflict_type", "unknown")
        resolution_strategy = self.resolution_strategies.get(conflict_type, self._resolve_generic_conflict)

        resolution = resolution_strategy(conflict_data)

        resolution["conflict_timestamp"] = datetime.now().isoformat()
        resolution["entity_type"] = "<EntityName>"

        self.conflict_history.append({
            "conflict": conflict_data,
            "resolution": resolution,
            "timestamp": datetime.now().isoformat()
        })

        logger.info(f"‚úÖ Conflict resolved: {resolution.get('resolution_status', 'unknown')}")

        return resolution

    def _resolve_dependency_conflict(self, conflict_data: Dict[str, Any]) -> Dict[str, Any]:
        """Resolve dependency conflicts between components"""
        conflicting_agents = conflict_data.get("conflicting_agents", [])
        dependency_issue = conflict_data.get("dependency_issue", "")

        resolution = {
            "resolution_type": "dependency_resolution",
            "resolution_status": "resolved",
            "resolution_strategy": "sequential_execution",
            "resolution_details": {
                "ordered_execution": [
                    "DatabaseSWEA.setup_database",
                    "BackendSWEA.generate_model",
                    "BackendSWEA.generate_api",
                    "FrontendSWEA.generate_ui",
                    "TestSWEA.generate_all_tests_with_collaboration"
                ],
                "dependency_management": "Explicit dependency ordering enforced",
                "conflict_prevention": "Sequential task execution"
            }
        }

        return resolution

    def _resolve_naming_conflict(self, conflict_data: Dict[str, Any]) -> Dict[str, Any]:
        """Resolve naming convention conflicts"""
        naming_issue = conflict_data.get("naming_issue", "")

        resolution = {
            "resolution_type": "naming_resolution",
            "resolution_status": "resolved",
            "resolution_strategy": "standardized_naming",
            "resolution_details": {
                "entity_naming": "<EntityName>",
                "table_naming": "<entity_name_lower>s",
                "api_naming": "/api/<entity_name_lower>s",
                "variable_naming": "<entity_name_lower>",
                "function_naming": "create_<entity_name_lower>, get_<entity_name_lower>s, etc."
            }
        }

        return resolution

    def _resolve_architecture_conflict(self, conflict_data: Dict[str, Any]) -> Dict[str, Any]:
        """Resolve architecture design conflicts"""
        architecture_issue = conflict_data.get("architecture_issue", "")

        resolution = {
            "resolution_type": "architecture_resolution",
            "resolution_status": "resolved",
            "resolution_strategy": "standardized_architecture",
            "resolution_details": {
                "database_architecture": "SQLAlchemy with declarative base",
                "api_architecture": "FastAPI with Pydantic models",
                "ui_architecture": "Streamlit with component-based design",
                "testing_architecture": "Pytest with comprehensive test suites"
            }
        }

        return resolution

    def _resolve_quality_conflict(self, conflict_data: Dict[str, Any]) -> Dict[str, Any]:
        """Resolve quality standard conflicts"""
        quality_issue = conflict_data.get("quality_issue", "")

        resolution = {
            "resolution_type": "quality_resolution",
            "resolution_status": "resolved",
            "resolution_strategy": "enforced_quality_standards",
            "resolution_details": {
                "code_quality_threshold": 0.8,
                "test_coverage_threshold": 90,
                "performance_threshold": 2.0,
                "security_threshold": 0.85,
                "quality_enforcement": "Mandatory quality gates"
            }
        }

        return resolution

    def _resolve_generic_conflict(self, conflict_data: Dict[str, Any]) -> Dict[str, Any]:
        """Resolve generic conflicts with standard approach"""
        resolution = {
            "resolution_type": "generic_resolution",
            "resolution_status": "resolved",
            "resolution_strategy": "standard_approach",
            "resolution_details": {
                "approach": "Follow established patterns and best practices",
                "documentation": "Ensure proper documentation and comments",
                "testing": "Comprehensive testing for all changes",
                "review": "Technical review before implementation"
            }
        }

        return resolution
```

## TASK INSTRUCTIONS

You are a TechLeadSWEA (Software Engineering Autonomous Agent) specialized in technical leadership and coordination. Your role is to coordinate system generation and manage quality gates for domain entities.

### RESPONSIBILITIES:
1. **System Coordination**: Coordinate SWEA agents and manage system generation workflow
2. **Quality Gate Management**: Define and enforce quality standards for all components
3. **Conflict Resolution**: Resolve technical conflicts between SWEA agents
4. **Architecture Decisions**: Make technical architecture decisions and enforce standards
5. **Review and Approval**: Review system components and approve for deployment

### REQUIREMENTS:
- Coordinate system generation with proper task dependencies
- Define quality gates and enforce quality standards
- Resolve technical conflicts between SWEA agents
- Make architecture decisions and enforce technical standards
- Review and approve system components for deployment
- Maintain comprehensive coordination history
- Ensure semantic coherence with business vocabulary
- Follow technical leadership best practices

### OUTPUT FORMAT:
Provide complete, production-ready coordination and quality management code that can be directly used in the managed system. Include all necessary imports, proper error handling, and comprehensive documentation.

### IMPORTANT NOTES:
- Use the examples above as templates, replacing placeholders with actual entity names
- Ensure all generated code follows technical leadership best practices
- Include comprehensive quality gate management
- Implement proper conflict resolution mechanisms
- Maintain consistency with the existing codebase structure
- Focus on coordination and quality assurance
- Ensure proper integration between all system components
