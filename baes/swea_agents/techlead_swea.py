import json
import logging
import os
from datetime import datetime
from typing import Any, Dict, List

from ..agents.base_agent import BaseAgent
from ..llm.openai_client import OpenAIClient

logger = logging.getLogger(__name__)

def is_debug_mode():
    """Check if debug mode is enabled"""
    return os.getenv("BAE_DEBUG", "0").lower() in ("1", "true", "on", "yes")


class TechLeadSWEA(BaseAgent):
    """
    Technical Lead Software Engineering Autonomous Agent responsible for:
    - Technical architecture decisions and technology stack management
    - Quality gate management and code review authority
    - SWEA coordination and conflict resolution
    - Performance, security, and technical standards enforcement
    - Continuous improvement and optimization oversight

    Acts as the technical governance layer between BAEs and implementation SWEAs.
    """

    def __init__(self):
        super().__init__("TechLeadSWEA", "Technical Leadership and Coordination Agent", "SWEA")
        self.llm_client = OpenAIClient()

        # Technical decision tracking
        self.architecture_decisions = {}
        self.quality_standards = {}
        self.performance_requirements = {}
        self.security_standards = {}

        # SWEA coordination state
        self.active_coordination_plans = {}
        self.swea_performance_metrics = {}
        self.conflict_resolution_history = []

        # Governance history tracking
        self.coordination_history = {}
        self.review_history = []
        self.test_coordination_history = {}

    # Supported task identifiers
    _SUPPORTED_TASKS = {
        "coordinate_system_generation": "_coordinate_system_generation",
        "review_and_approve": "_review_and_approve",
        "resolve_technical_conflict": "_resolve_technical_conflict",
        "optimize_architecture": "_optimize_architecture",
        "manage_quality_gate": "_manage_quality_gate",
        "coordinate_test_fixes": "_coordinate_test_fixes",
        "make_tech_decision": "_make_tech_decision",
        "assess_system_health": "_assess_system_health",
        "hybrid_coordination": "_hybrid_coordination",
    }

    def handle_task(self, task: str, payload: Dict[str, Any]) -> Dict[str, Any]:
        """Handle technical leadership tasks with comprehensive governance"""
        if task == "coordinate_system_generation":
            return self._coordinate_system_generation(payload)
        elif task == "review_and_approve":
            return self._review_and_approve(payload)
        elif task == "resolve_technical_conflict":
            return self._resolve_technical_conflict(payload)
        elif task == "coordinate_test_fixes":
            return self._coordinate_test_fixes(payload)
        elif task == "make_tech_decision":
            return self._make_tech_decision(payload)
        elif task == "assess_system_health":
            return self._assess_system_health(payload)
        elif task == "optimize_architecture":
            return self._optimize_architecture(payload)
        elif task == "manage_quality_gate":
            return self._manage_quality_gate(payload)
        elif task == "hybrid_coordination":
            return self._hybrid_coordination(payload)
        else:
            return {"error": f"Unknown TechLeadSWEA task: {task}", "success": False}

    # ------------------------------------------------------------------
    # Core Coordination Methods
    # ------------------------------------------------------------------

    def _coordinate_system_generation(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """Coordinate complete system generation with technical governance"""
        try:
            entity = payload.get("entity", "Unknown")
            attributes = payload.get("attributes", [])
            context = payload.get("context", "")
            is_evolution = payload.get("is_evolution", False)
            business_requirements = payload.get("business_requirements", {})

            if is_debug_mode():
                logger.info("ðŸ§  TechLeadSWEA: Coordinating system generation for %s entity", entity)

            # Analyze technical requirements and create enhanced coordination plan
            technical_analysis = self._analyze_technical_requirements(
                entity, attributes, context, is_evolution
            )

            # Define quality gates based on business requirements
            quality_gates = self._define_quality_gates(business_requirements, is_evolution)

            # Create enhanced coordination plan with technical governance
            enhanced_plan = self._create_enhanced_coordination_plan(
                entity, attributes, context, is_evolution, technical_analysis, quality_gates
            )

            # Store coordination state for tracking
            coordination_id = f"coord_{entity}_{self._get_timestamp()}"
            self.coordination_history[coordination_id] = {
                "entity": entity,
                "attributes": attributes,
                "context": context,
                "is_evolution": is_evolution,
                "technical_analysis": technical_analysis,
                "quality_gates": quality_gates,
                "enhanced_plan": enhanced_plan,
                "status": "coordinating",
                "created_at": self._get_timestamp(),
            }

            # Simple decision log
            self._log_decision("coordination", entity, "APPROVED", 
                             f"{len(enhanced_plan)} SWEA tasks scheduled",
                             type="evolution" if is_evolution else "creation",
                             attributes=len(attributes),
                             quality_gates=len(quality_gates))

            return {
                "success": True,
                "data": {
                    "coordination_id": coordination_id,
                    "enhanced_coordination_plan": enhanced_plan,
                    "technical_analysis": technical_analysis,
                    "quality_gates": quality_gates,
                    "governance_strategy": "centralized_technical_oversight",
                    "approval_workflow": "sequential_with_quality_gates",
                },
                "message": f"Technical coordination plan established for {entity}",
                "technical_governance": True,
            }

        except Exception as e:
            logger.error("âŒ TechLeadSWEA coordination failed: %s", str(e))
            return {
                "success": False,
                "error": f"Technical coordination failed: {str(e)}",
                "technical_governance": False,
            }

    def _coordinate_test_fixes(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """
        Coordinate test failure resolution across SWEA agents
        Enhanced with detailed failure analysis and specific fix routing
        """
        try:
            entity = payload.get("entity", "Unknown")
            test_failures = payload.get("test_failures", [])
            coordination_id = payload.get("coordination_id", f"test_fix_{entity}")
            
            # NEW: Extract detailed failure context from test execution
            failure_context = payload.get("failure_context", {})
            test_execution = failure_context.get("test_execution", {})
            stderr = test_execution.get("stderr", "")
            stdout = test_execution.get("stdout", "")
            exit_code = test_execution.get("exit_code", -1)

            logger.info("ðŸ§  TechLeadSWEA: Coordinating test fixes for %s", entity)
            logger.info("   ðŸ“Š Test Context: %d failures, exit_code=%s", len(test_failures), exit_code)
            if stderr:
                logger.info("   ðŸ“ Key errors: %s", stderr[:200])  # First 200 chars

            coordination_log = []
            fix_decisions = []

            # ENHANCED: Analyze actual test execution output for specific issues
            specific_issues = self._analyze_detailed_test_failures(stderr, stdout, entity)
            coordination_log.append(f"ðŸ” Analyzed test output - found {len(specific_issues)} specific issues")
            
            # Create targeted fix decisions based on specific issues
            for issue in specific_issues:
                fix_decision = {
                    "swea_agent": issue["responsible_swea"],
                    "responsible_swea": issue["responsible_swea"],
                    "fix_actions": issue["fix_actions"],
                    "priority": issue["priority"],
                    "reasoning": issue["description"],
                    "confidence": issue["confidence"],
                    "specific_issue": issue["issue_type"],
                    "detailed_context": issue.get("context", {}),
                    "recommended_action": issue["fix_actions"][0] if issue["fix_actions"] else "analyze_and_fix"
                }
                fix_decisions.append(fix_decision)
                coordination_log.append(f"ðŸ“‹ {issue['issue_type']} â†’ {issue['responsible_swea']}: {issue['description']}")

            # Fallback: If no specific issues found, analyze generic test failures
            if not fix_decisions and test_failures:
                coordination_log.append("ðŸ”„ No specific issues found, analyzing generic test failures")
                for i, failure in enumerate(test_failures):
                    failure_category = failure.get("category", "unknown")
                    stderr_content = failure.get("stderr", "")
                    
                    if "import" in failure_category.lower() or "modulenotfounderror" in stderr_content.lower():
                        fix_decision = {
                            "swea_agent": "BackendSWEA",
                            "responsible_swea": "BackendSWEA",
                            "fix_actions": ["fix_import_dependencies", "regenerate_model_with_imports"],
                            "priority": "high",
                            "reasoning": f"Import error in test failure {i+1}",
                            "confidence": 0.8,
                            "recommended_action": "fix_import_dependencies"
                        }
                    elif "api" in failure_category.lower() or "404" in stderr_content:
                        fix_decision = {
                            "swea_agent": "BackendSWEA", 
                            "responsible_swea": "BackendSWEA",
                            "fix_actions": ["fix_api_routing", "regenerate_api_endpoints"],
                            "priority": "high",
                            "reasoning": f"API error in test failure {i+1}",
                            "confidence": 0.8,
                            "recommended_action": "fix_api_routing"
                        }
                    elif "assertion" in failure_category.lower():
                        fix_decision = {
                            "swea_agent": "TestSWEA",
                            "responsible_swea": "TestSWEA",
                            "fix_actions": ["review_test_assertions", "fix_test_expectations"],
                            "priority": "medium",
                            "reasoning": f"Test assertion error in failure {i+1}",
                            "confidence": 0.7,
                            "recommended_action": "review_test_assertions"
                        }
                    else:
                        fix_decision = {
                            "swea_agent": "TestSWEA",
                            "responsible_swea": "TestSWEA",
                            "fix_actions": ["analyze_test_failure", "fix_test_issues"],
                            "priority": "medium",
                            "reasoning": f"Unknown test failure {i+1}",
                            "confidence": 0.5,
                            "recommended_action": "analyze_test_failure"
                        }
                    
                    fix_decisions.append(fix_decision)
                    coordination_log.append(f"Analyzed failure {i+1}: {failure_category} â†’ {fix_decision['swea_agent']}")

            # Create coordination result with enhanced information
            coordination_result = {
                "entity": entity,
                "coordination_id": coordination_id,
                "fix_decisions": fix_decisions,
                "coordination_log": coordination_log,
                "total_failures": len(test_failures),
                "fixes_planned": len(fix_decisions),
                "specific_issues_found": len(specific_issues),
                "failure_analysis": {
                    "stderr_summary": stderr[:500] if stderr else "",
                    "exit_code": exit_code,
                    "issues_detected": [issue["issue_type"] for issue in specific_issues]
                }
            }

            # Enhanced decision logging
            swea_distribution = {}
            for decision in fix_decisions:
                swea = decision["swea_agent"] 
                swea_distribution[swea] = swea_distribution.get(swea, 0) + 1
            
            self._log_decision("test_fix_coordination", entity, "COORDINATED", 
                             f"{len(fix_decisions)} specific fixes planned",
                             failures_analyzed=len(test_failures),
                             specific_issues=len(specific_issues),
                             swea_assignments=", ".join(f"{s}:{c}" for s, c in swea_distribution.items()))

            response = self.create_success_response("coordinate_test_fixes", coordination_result)
            response["technical_governance"] = True
            return response

        except Exception as e:
            logger.error(f"âŒ TechLeadSWEA test fix coordination failed: {str(e)}")
            return self.create_error_response("coordinate_test_fixes", str(e), "coordination_error")

    def _analyze_detailed_test_failures(self, stderr: str, stdout: str, entity: str) -> List[Dict[str, Any]]:
        """
        Analyze detailed test execution output to identify specific issues and route fixes appropriately
        """
        issues = []
        combined_output = f"{stderr} {stdout}".lower()
        
        # 1. Mock object issues (common in generated tests)
        if "magicmock" in combined_output or "mock object" in combined_output:
            issues.append({
                "issue_type": "mock_configuration_error",
                "responsible_swea": "TestSWEA",
                "fix_actions": ["fix_test_mocking", "update_test_configuration"],
                "priority": "high",
                "confidence": 0.9,
                "description": "Test mocking configuration issues - tests expecting real data but getting Mock objects",
                "context": {"error_pattern": "mock_object_validation"}
            })
        
        # 2. Missing function errors (UI tests)
        if "nameerror: name" in combined_output and ("display_" in combined_output or "create_" in combined_output or "edit_" in combined_output):
            issues.append({
                "issue_type": "missing_ui_functions",
                "responsible_swea": "FrontendSWEA",
                "fix_actions": ["regenerate_ui_with_functions", "add_missing_streamlit_functions"],
                "priority": "high",
                "confidence": 0.9,
                "description": "Missing UI functions in Streamlit application - tests reference functions that don't exist",
                "context": {"error_pattern": "missing_functions"}
            })
        
        # 3. API status code mismatches
        if "assert" in combined_output and ("200 == 422" in combined_output or "200 == 404" in combined_output):
            issues.append({
                "issue_type": "api_validation_mismatch",
                "responsible_swea": "BackendSWEA",
                "fix_actions": ["fix_api_validation", "update_error_handling"],
                "priority": "high",
                "confidence": 0.8,
                "description": "API validation errors - endpoints returning wrong status codes",
                "context": {"error_pattern": "status_code_mismatch"}
            })
        
        # 4. Pydantic validation errors
        if "validationerror" in combined_output and "input should be a valid string" in combined_output:
            issues.append({
                "issue_type": "pydantic_validation_error",
                "responsible_swea": "BackendSWEA",
                "fix_actions": ["fix_model_validation", "update_pydantic_models"],
                "priority": "high",
                "confidence": 0.8,
                "description": "Pydantic model validation errors - incorrect data types or validation rules",
                "context": {"error_pattern": "pydantic_validation"}
            })
        
        # 5. Database connection/query issues
        if "sqlite" in combined_output and ("fetchone" in combined_output or "fetchall" in combined_output):
            issues.append({
                "issue_type": "database_query_error",
                "responsible_swea": "DatabaseSWEA",
                "fix_actions": ["fix_database_queries", "update_schema"],
                "priority": "medium",
                "confidence": 0.7,
                "description": "Database query issues - problems with SQL queries or database connection",
                "context": {"error_pattern": "database_query"}
            })
        
        # 6. Import/module errors
        if "modulenotfounderror" in combined_output or "importerror" in combined_output:
            issues.append({
                "issue_type": "import_dependency_error",
                "responsible_swea": "BackendSWEA",
                "fix_actions": ["fix_imports", "update_dependencies"],
                "priority": "high",
                "confidence": 0.9,
                "description": "Import or module dependency errors - missing or incorrect imports",
                "context": {"error_pattern": "import_error"}
            })
        
        # 7. Syntax errors
        if "syntaxerror" in combined_output:
            issues.append({
                "issue_type": "syntax_error",
                "responsible_swea": "BackendSWEA",
                "fix_actions": ["fix_syntax_errors", "regenerate_code"],
                "priority": "critical",
                "confidence": 0.95,
                "description": "Syntax errors in generated code - code compilation failures",
                "context": {"error_pattern": "syntax_error"}
            })
        
        return issues

    def _review_and_approve(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """Review SWEA outputs and make approval decisions with quality gate enforcement"""
        try:
            entity = payload.get("entity", "Unknown")
            swea_agent = payload.get("swea_agent", "")
            task_type = payload.get("task_type", "")
            result = payload.get("result", {})
            quality_gates = payload.get("quality_gates", {})
            final_review = payload.get("final_review", False)
            retry_count = payload.get("retry_count", 0)

            if final_review:
                return self._conduct_final_system_review(payload)

            logger.info("ðŸ§  TechLeadSWEA: Reviewing %s.%s for %s (retry: %d)", 
                       swea_agent, task_type, entity, retry_count)

            # Comprehensive quality assessment
            quality_assessment = self._assess_component_quality(
                swea_agent, task_type, result, quality_gates
            )

            # Technical standards compliance check
            compliance_check = self._check_technical_compliance(swea_agent, task_type, result)

            # Business alignment validation
            business_alignment = self._validate_business_alignment(entity, result, swea_agent)

            # Overall approval decision - consistent validation regardless of retry count
            overall_approval = (
                quality_assessment.get("meets_standards", False)
                and compliance_check.get("compliant", False)
                and business_alignment.get("aligned", False)
            )

            # Collect technical feedback for improvement
            technical_feedback = []
            if not quality_assessment.get("meets_standards", False):
                technical_feedback.extend(quality_assessment.get("issues", []))
            if not compliance_check.get("compliant", False):
                technical_feedback.extend(compliance_check.get("violations", []))
            if not business_alignment.get("aligned", False):
                technical_feedback.extend(business_alignment.get("misalignments", []))

            # Add context-specific feedback for retries
            if retry_count > 0 and not overall_approval:
                technical_feedback.append(f"This is retry attempt {retry_count} - please focus on addressing core issues")

            # Calculate quality score
            quality_score = self._calculate_quality_score(
                quality_assessment, compliance_check, business_alignment
            )

            # Record review decision with retry context
            review_record = {
                "entity": entity,
                "swea_agent": swea_agent,
                "task_type": task_type,
                "quality_score": quality_score,
                "overall_approval": overall_approval,
                "technical_feedback": technical_feedback,
                "reviewed_at": self._get_timestamp(),
                "reviewer": "TechLeadSWEA",
                "retry_count": retry_count,
            }
            self.review_history.append(review_record)

            # Simple decision log
            self._log_decision("review", entity, "APPROVED" if overall_approval else "REJECTED",
                             f"quality score {quality_score:.2f}",
                             component=f"{swea_agent}.{task_type}",
                             retry_attempt=retry_count,
                             issues_found=len(technical_feedback) if technical_feedback else 0)

            feedback = []
            if not quality_assessment.get("meets_standards", False):
                feedback.extend(quality_assessment.get("issues", []))
            if not compliance_check.get("compliant", False):
                feedback.extend(compliance_check.get("violations", []))
            if not business_alignment.get("aligned", False):
                feedback.extend(business_alignment.get("misalignments", []))

            return {
                "success": True,
                "data": {
                    "overall_approval": overall_approval,
                    "quality_score": quality_score,
                    "technical_feedback": technical_feedback,
                    "entity": entity,
                    "feedback": feedback,
                    "retry_context": {
                        "retry_count": retry_count,
                    }
                },
                "message": f"Review completed for {swea_agent}.{task_type} (retry: {retry_count})",
                "technical_governance": True,
            }

        except Exception as e:
            logger.error("âŒ TechLeadSWEA review failed: %s", str(e))
            return {
                "success": False,
                "error": f"Technical review failed: {str(e)}",
                "overall_approval": False,
                "technical_governance": False,
            }

    def _resolve_technical_conflict(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """Resolve conflicts between SWEA agents with technical authority"""
        try:
            conflict_type = payload.get("conflict_type", "resource_conflict")
            involved_sweas = payload.get("involved_sweas", [])
            conflict_details = payload.get("conflict_details", {})
            entity = payload.get("entity", "Unknown")

            logger.info("ðŸ§  TechLeadSWEA: Resolving technical conflict (%s)", conflict_type)

            # Analyze conflict and make technical decision
            resolution_strategy = self._determine_conflict_resolution_strategy(
                conflict_type, involved_sweas, conflict_details
            )

            # Create conflict resolution plan
            resolution_plan = {
                "strategy": resolution_strategy,
                "priority_assignments": self._assign_swea_priorities(involved_sweas, conflict_details),
                "technical_constraints": self._define_technical_constraints(conflict_details),
                "resolution_timeline": "immediate",
            }

            # Record conflict resolution for future reference
            self.conflict_resolution_history.append(
                {
                    "conflict_type": conflict_type,
                    "involved_sweas": involved_sweas,
                    "resolution_strategy": resolution_strategy,
                    "resolved_at": self._get_timestamp(),
                    "entity": entity,
                }
            )

            # Simple decision log
            self._log_decision("conflict_resolution", entity, "RESOLVED", 
                             f"{resolution_strategy} strategy",
                             conflict_type=conflict_type,
                             involved_sweas=", ".join(involved_sweas),
                             constraints=len(resolution_plan["technical_constraints"]))

            return {
                "success": True,
                "data": {
                    "resolution_strategy": resolution_strategy,
                    "resolution_plan": resolution_plan,
                    "conflict_type": conflict_type,
                    "technical_authority_exercised": True,
                },
                "message": f"Technical conflict resolved: {conflict_type}",
                "technical_governance": True,
            }

        except Exception as e:
            logger.error("âŒ TechLeadSWEA conflict resolution failed: %s", str(e))
            return {
                "success": False,
                "error": f"Conflict resolution failed: {str(e)}",
                "technical_governance": False,
            }

    # ------------------------------------------------------------------
    # Technical Decision Making Methods
    # ------------------------------------------------------------------

    def _make_architecture_decisions(
        self, entity: str, business_requirements: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Make architecture decisions with comprehensive analysis"""
        try:
            logger.info("ðŸ§  TechLeadSWEA: Making architecture decisions for %s", entity)

            # Analyze business requirements for technical implications
            technical_requirements = self._extract_technical_requirements(business_requirements)

            # Define architecture patterns based on requirements
            architecture_patterns = self._select_architecture_patterns(
                entity, technical_requirements
            )

            # Technology stack recommendations
            tech_stack = self._recommend_technology_stack(entity, technical_requirements)

            # Performance requirements
            performance_specs = self._define_performance_specifications(technical_requirements)

            # Security requirements
            security_specs = self._define_security_specifications(technical_requirements)

            # Store architecture decisions
            architecture_decision = {
                "entity": entity,
                "patterns": architecture_patterns,
                "tech_stack": tech_stack,
                "performance_specs": performance_specs,
                "security_specs": security_specs,
                "business_requirements": business_requirements,
                "decided_at": self._get_timestamp(),
            }

            self.architecture_decisions[entity] = architecture_decision

            # Simple decision log
            self._log_decision("architecture", entity, "APPROVED", 
                             f"{tech_stack.get('primary_tech', 'standard')} stack",
                             patterns=", ".join(architecture_patterns),
                             performance=performance_specs.get("target_level", "standard"),
                             security=security_specs.get("security_level", "standard"))

            return {
                "architecture_patterns": architecture_patterns,
                "tech_stack": tech_stack,
                "performance_specs": performance_specs,
                "security_specs": security_specs,
                "technical_rationale": f"Architecture optimized for {entity} domain requirements",
                "governance_applied": True,
            }

        except Exception as e:
            logger.error("âŒ TechLeadSWEA architecture decision failed: %s", str(e))
            return {
                "error": f"Architecture decision failed: {str(e)}",
                "governance_applied": False,
            }

    def _analyze_test_failure_and_decide(
        self, failure: Dict[str, Any], entity: str
    ) -> Dict[str, Any]:
        """Analyze test failure and make technical decision on how to fix it."""

        # failure_type = failure.get("category", "unknown")  # Currently unused
        error_details = failure.get("stderr", "") + " " + failure.get("stdout", "")

        # Technical decision matrix based on failure analysis
        if "ModuleNotFoundError" in error_details or "ImportError" in error_details:
            decision = {
                "issue_type": "dependency_management",
                "responsible_swea": "BackendSWEA",
                "recommended_action": "fix_dependencies",
                "priority": "high",
                "technical_rationale": "Missing dependencies block all functionality",
                "fix_context": {"error_details": error_details, "issue_type": "missing_dependency"},
            }
        elif "ValidationError" in error_details or "pydantic" in error_details.lower():
            if "empty" in error_details.lower() or "length" in error_details.lower():
                decision = {
                    "issue_type": "business_validation_mismatch",
                    "responsible_swea": "BackendSWEA",
                    "recommended_action": "update_model_validation",
                    "priority": "medium",
                    "technical_rationale": "Business rules should be enforced at model level",
                    "fix_context": {
                        "error_details": error_details,
                        "issue_type": "validation_rule",
                    },
                }
            else:
                decision = {
                    "issue_type": "test_expectation_mismatch",
                    "responsible_swea": "TestSWEA",
                    "recommended_action": "update_test_expectations",
                    "priority": "medium",
                    "technical_rationale": "Test expectations should match current Pydantic v2 behavior",
                    "fix_context": {
                        "error_details": error_details,
                        "issue_type": "test_expectation",
                    },
                }
        elif "SyntaxError" in error_details or "IndentationError" in error_details:
            decision = {
                "issue_type": "code_generation_quality",
                "responsible_swea": "BackendSWEA",
                "recommended_action": "fix_code_syntax",
                "priority": "high",
                "technical_rationale": "Syntax errors indicate code generation issues",
                "fix_context": {"error_details": error_details, "issue_type": "syntax_error"},
            }
        else:
            decision = {
                "issue_type": "general_technical_issue",
                "responsible_swea": "BackendSWEA",  # Default to backend for unknown issues
                "recommended_action": "investigate_and_fix",
                "priority": "medium",
                "technical_rationale": "Unknown issues require investigation",
                "fix_context": {"error_details": error_details, "issue_type": "unknown"},
            }

        # Simple decision log
        self._log_decision("test_failure_analysis", entity, "ANALYZED", 
                         decision["technical_rationale"],
                         issue_type=decision["issue_type"],
                         responsible_swea=decision["responsible_swea"],
                         priority=decision["priority"])

        return decision

    def _create_swea_fix_task(
        self, fix_decision: Dict[str, Any], failure: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Create a specific task for a SWEA to fix an issue."""
        return {
            "responsible_swea": fix_decision["responsible_swea"],
            "task_type": fix_decision["recommended_action"],
            "task_description": f"Fix {fix_decision['issue_type']} issue",
            "priority": fix_decision["priority"],
            "context": fix_decision.get("fix_context", {}),
            "original_failure": failure,
            "technical_requirements": self._get_fix_requirements(fix_decision),
        }

    def _get_fix_requirements(self, fix_decision: Dict[str, Any]) -> Dict[str, Any]:
        """Get specific technical requirements for fixing an issue."""
        issue_type = fix_decision.get("issue_type", "")

        if issue_type == "dependency_management":
            return {
                "update_requirements_txt": True,
                "install_missing_packages": True,
                "validate_compatibility": True,
            }
        elif issue_type == "business_validation_mismatch":
            return {
                "add_field_validation": True,
                "preserve_business_rules": True,
                "maintain_backward_compatibility": True,
            }
        elif issue_type == "test_expectation_mismatch":
            return {
                "update_error_message_expectations": True,
                "align_with_pydantic_v2": True,
                "maintain_test_coverage": True,
            }

        return {"investigate_thoroughly": True, "document_changes": True}

    def _define_next_steps(self, fix_decisions: List[Dict[str, Any]]) -> List[str]:
        """Define next steps after coordinating test fixes."""
        next_steps = []

        high_priority_count = sum(1 for decision in fix_decisions if decision["priority"] == "high")
        if high_priority_count > 0:
            next_steps.append(f"Address {high_priority_count} high-priority issues immediately")

        swea_groups = {}
        for decision in fix_decisions:
            swea = decision["responsible_swea"]
            if swea not in swea_groups:
                swea_groups[swea] = []
            swea_groups[swea].append(decision["issue_type"])

        for swea, issues in swea_groups.items():
            next_steps.append(f"{swea} should address: {', '.join(issues)}")

        next_steps.append("Re-run tests after all fixes are applied")
        next_steps.append("TechLeadSWEA will review and approve fixes")

        return next_steps

    def _create_enhanced_coordination_plan(
        self,
        entity: str,
        attributes: List[str],
        context: str,
        is_evolution: bool,
        technical_analysis: Dict[str, Any],
        quality_gates: Dict[str, Any],
    ) -> List[Dict[str, Any]]:
        """Create enhanced coordination plan with technical governance"""
        base_plan = [
            {
                "swea_agent": "DatabaseSWEA",
                "task_type": "setup_database" if not is_evolution else "migrate_schema",
                "payload": {
                    "entity": entity,
                    "attributes": attributes,
                    "context": context,
                    "preserve_data": is_evolution,
                    "business_rules": True,
                },
                "priority": 1,
                "requires_approval": True,
                "technical_requirements": technical_analysis.get("database_requirements", {}),
                "quality_criteria": quality_gates.get("business_alignment", {}),
            },
            {
                "swea_agent": "BackendSWEA",
                "task_type": "generate_model",
                "payload": {
                    "entity": entity,
                    "attributes": attributes,
                    "context": context,
                    "domain_focus": True,
                    "semantic_coherence": True,
                },
                "priority": 2,
                "requires_approval": True,
                "technical_requirements": technical_analysis.get("api_requirements", {}),
                "quality_criteria": quality_gates.get("code_quality", {}),
            },
            {
                "swea_agent": "BackendSWEA",
                "task_type": "generate_api",
                "payload": {
                    "entity": entity,
                    "attributes": attributes,
                    "context": context,
                    "crud_operations": True,
                    "business_vocabulary": True,
                },
                "priority": 3,
                "requires_approval": True,
                "technical_requirements": technical_analysis.get("api_requirements", {}),
                "quality_criteria": quality_gates.get("performance", {}),
            },
            {
                "swea_agent": "FrontendSWEA",
                "task_type": "generate_ui",
                "payload": {
                    "entity": entity,
                    "attributes": attributes,
                    "context": context,
                    "business_vocabulary": True,
                    "user_friendly": True,
                },
                "priority": 4,
                "requires_approval": True,
                "technical_requirements": technical_analysis.get("ui_requirements", {}),
                "quality_criteria": quality_gates.get("business_alignment", {}),
            },
            {
                "swea_agent": "TestSWEA",
                "task_type": "generate_all_tests_with_collaboration",
                "payload": {
                    "entity": entity,
                    "attributes": attributes,
                    "context": context,
                    "test_types": ["unit", "integration", "api"],
                    "coverage_target": "comprehensive",
                },
                "priority": 5,
                "requires_approval": True,
                "technical_requirements": technical_analysis.get("testing_requirements", {}),
                "quality_criteria": quality_gates.get("test_coverage", {}),
            },
        ]

        return base_plan

    def _define_quality_gates(
        self, business_requirements: Dict[str, Any], is_evolution: bool
    ) -> Dict[str, Any]:
        """Define quality gates based on business requirements"""
        return {
            "code_quality": {
                "min_score": 8.0,
                "linting": "strict",
                "formatting": "black_compliant",
                "complexity": "low_to_medium",
            },
            "test_coverage": {"minimum": 85, "target": 95, "critical_paths": 100},
            "performance": {
                "api_response_time": 200,  # ms
                "ui_load_time": 2000,  # ms
                "database_query_time": 100,  # ms
            },
            "security": {
                "input_validation": "comprehensive",
                "sql_injection_protection": "enabled",
                "xss_protection": "enabled",
            },
            "business_alignment": {
                "vocabulary_preservation": "strict",
                "domain_coherence": "high",
                "semantic_consistency": "enforced",
            },
        }

    def _analyze_technical_requirements(
        self, entity: str, attributes: List[str], context: str, is_evolution: bool
    ) -> Dict[str, Any]:
        """Analyze technical requirements for system generation"""
        return {
            "complexity_level": "medium" if len(attributes) <= 5 else "high",
            "database_requirements": {
                "tables_needed": 1,
                "relationships": "minimal" if not is_evolution else "preserve_existing",
                "indexes": ["primary_key", "business_key"],
                "constraints": ["not_null", "unique_business_key"],
            },
            "api_requirements": {
                "endpoints": 5,  # CRUD + list
                "authentication": "basic",
                "validation": "comprehensive",
                "error_handling": "production_ready",
            },
            "ui_requirements": {
                "components": ["list", "create", "edit", "delete"],
                "responsiveness": "mobile_ready",
                "accessibility": "wcag_aa",
            },
            "testing_requirements": {
                "coverage_target": 90,
                "test_types": ["unit", "integration", "api"],
                "automation": "full",
            },
            "performance_targets": {
                "api_response": "< 200ms",
                "ui_load": "< 2s",
                "database_query": "< 100ms",
            },
        }

    # ------------------------------------------------------------------
    # Helper Methods
    # ------------------------------------------------------------------

    def _review_artifact(self, artifact: Dict[str, Any], entity: str) -> Dict[str, Any]:
        """Review a single artifact for quality and compliance."""

        artifact_type = artifact.get("type", "unknown")
        artifact_content = artifact.get("content", "")

        # Basic quality checks
        syntax_valid = (
            "class " in artifact_content or "def " in artifact_content or "@" in artifact_content
        )
        has_documentation = '"""' in artifact_content or "# " in artifact_content
        follows_naming_conventions = entity.lower() in artifact_content.lower()

        # Calculate approval score
        quality_checks = {
            "syntax_valid": syntax_valid,
            "has_documentation": has_documentation,
            "follows_naming_conventions": follows_naming_conventions,
            "content_length_adequate": len(artifact_content) > 100,
        }

        approval_score = sum(quality_checks.values()) / len(quality_checks)
        approved = approval_score >= 0.75  # 75% quality threshold

        feedback = []
        if not syntax_valid:
            feedback.append("Improve code syntax and structure")
        if not has_documentation:
            feedback.append("Add comprehensive documentation")
        if not follows_naming_conventions:
            feedback.append(f"Ensure naming conventions include {entity} entity reference")

        return {
            "artifact_type": artifact_type,
            "approved": approved,
            "quality_score": approval_score,
            "quality_checks": quality_checks,
            "feedback": feedback,
        }

    def _determine_next_actions(
        self, overall_approval: bool, technical_feedback: List[str]
    ) -> List[str]:
        """Determine next actions based on review results"""
        if overall_approval:
            return ["Proceed with deployment", "Monitor system performance"]
        else:
            actions = ["Address technical feedback", "Re-run quality gates"]
            if technical_feedback:
                actions.extend([f"Fix: {feedback}" for feedback in technical_feedback[:3]])
            return actions

    def _analyze_test_failures(self, test_failures: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze test failures and categorize issues"""
        categories = {}
        strategy = "sequential_fixes"

        for failure in test_failures:
            category = failure.get("category", "unknown")
            if category not in categories:
                categories[category] = []
            categories[category].append(failure)

        return {
            "categories": categories,
            "total_failures": len(test_failures),
            "strategy": strategy,
            "priority": "high" if len(test_failures) > 5 else "medium",
        }

    def _create_fix_decisions(
        self, failure_analysis: Dict[str, Any], entity: str
    ) -> List[Dict[str, Any]]:
        """Create fix decisions with SWEA assignments"""
        fix_decisions = []
        categories = failure_analysis.get("categories", {})

        for category, failures in categories.items():
            if category == "import_error":
                fix_decisions.append(
                    {
                        "issue_type": "import_error",
                        "responsible_swea": "BackendSWEA",
                        "recommended_action": "fix_import_paths",
                        "priority": "high",
                        "estimated_effort": "low",
                    }
                )
            elif category == "test_execution_failure":
                fix_decisions.append(
                    {
                        "issue_type": "test_execution_failure",
                        "responsible_swea": "TestSWEA",
                        "recommended_action": "fix_test_configuration",
                        "priority": "high",
                        "estimated_effort": "medium",
                    }
                )
            elif category == "api_error":
                fix_decisions.append(
                    {
                        "issue_type": "api_error",
                        "responsible_swea": "BackendSWEA",
                        "recommended_action": "fix_api_endpoints",
                        "priority": "medium",
                        "estimated_effort": "medium",
                    }
                )
            else:
                fix_decisions.append(
                    {
                        "issue_type": category,
                        "responsible_swea": "BackendSWEA",
                        "recommended_action": "general_fix",
                        "priority": "medium",
                        "estimated_effort": "medium",
                    }
                )

        return fix_decisions

    def _estimate_fix_time(self, fix_decisions: List[Dict[str, Any]]) -> int:
        """Estimate total fix time in minutes"""
        effort_map = {"low": 5, "medium": 15, "high": 30}
        total_time = 0

        for decision in fix_decisions:
            effort = decision.get("estimated_effort", "medium")
            total_time += effort_map.get(effort, 15)

        return total_time

    def _assess_component_quality(
        self, swea_agent: str, task_type: str, result: Dict[str, Any], quality_gates: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Assess component quality with robust validation to prevent infinite retry loops"""
        meets_standards = True
        issues = []

        # Get result data
        data = result.get("data", {})

        if "Backend" in swea_agent:
            if "generate_model" in task_type:
                # BackendSWEA returns "code" not "model_code"
                if not data.get("code"):
                    meets_standards = False
                    issues.append("Missing model code generation")
                else:
                    # Validate that it looks like a Pydantic model
                    code = data.get("code", "")
                    if "BaseModel" not in code or "class " not in code:
                        meets_standards = False
                        issues.append("Generated code doesn't appear to be a valid Pydantic model")
                        
            elif "generate_api" in task_type:
                # BackendSWEA returns "code" not "api_code"
                if not data.get("code"):
                    meets_standards = False
                    issues.append("Missing API code generation")
                else:
                    # Validate that it looks like FastAPI code
                    code = data.get("code", "")
                    if "APIRouter" not in code or "from fastapi import" not in code:
                        meets_standards = False
                        issues.append("Generated code doesn't appear to be valid FastAPI routes")

        elif "Frontend" in swea_agent:
            # Enhanced FrontendSWEA validation - more lenient to prevent infinite retry loops
            if not data.get("code"):
                meets_standards = False
                issues.append("Missing UI code generation")
            else:
                # Simplified Streamlit validation with basic requirements
                code = data.get("code", "")
                code_lower = code.lower()
                
                # Basic Streamlit validation - require at least one clear indicator
                has_streamlit_import = any([
                    "import streamlit" in code_lower,
                    "from streamlit" in code_lower,
                    "streamlit" in code_lower
                ])
                
                has_streamlit_usage = any([
                    "st." in code,
                    "st.title" in code_lower,
                    "st.write" in code_lower,
                    "st.dataframe" in code_lower,
                    "st.form" in code_lower
                ])
                
                has_main_function = "def main()" in code_lower
                
                # Require basic Streamlit structure
                if not has_streamlit_import:
                    meets_standards = False
                    issues.append("Missing Streamlit import")
                elif not (has_streamlit_usage or has_main_function):
                    meets_standards = False
                    issues.append("Missing Streamlit usage or main function")
                elif len(code.strip()) < 50:  # Very minimal code length check
                    meets_standards = False
                    issues.append("Generated UI code appears too minimal")
                else:
                    # Code meets basic requirements
                    logger.debug(f"TechLeadSWEA: FrontendSWEA code validation passed - has_import={has_streamlit_import}, has_usage={has_streamlit_usage}, has_main={has_main_function}")

        elif "Database" in swea_agent:
            # DatabaseSWEA validation - appropriate for database operations
            if "setup_database" in task_type:
                # Check for database path or creation confirmation
                database_path = data.get("database_path")
                tables_created = data.get("tables_created", [])
                
                if not database_path and not tables_created:
                    meets_standards = False
                    issues.append("Missing database creation confirmation")
                else:
                    # Validate database setup details
                    if database_path and not database_path.endswith('.db'):
                        meets_standards = False
                        issues.append("Invalid database file path format")
                    
                    if not tables_created or len(tables_created) == 0:
                        meets_standards = False
                        issues.append("No database tables were created")
            
            elif "migrate_schema" in task_type:
                # Check for migration confirmation
                migration_applied = data.get("migration_applied", False)
                if not migration_applied:
                    meets_standards = False
                    issues.append("Schema migration was not applied successfully")

        elif "Test" in swea_agent:
            # TestSWEA validation logic considers two situations:
            #   1. Phase-1 test generation (tests_generated flag present)
            #   2. Phase-2 validation run (test_execution_result present) â€“ must be 100 %.
            if "generate" in task_type:
                # Phase-1 â€“ test files are generated but not yet executed.
                if result.get("tests_generated") or data.get("tests_generated"):
                    # Accept generation step; execution will be validated later.
                    logger.info("âœ… TechLeadSWEA: Test files generated â€“ execution deferred to Phase 2")
                    test_execution_result = None
                else:
                    test_execution_result = result.get("test_execution_result")
                
                if not result.get("tests_generated") and not test_execution_result:
                    meets_standards = False
                    issues.append("Missing test execution results - tests were not executed")
                    logger.warning("ðŸ” TechLeadSWEA: TestSWEA task missing test execution results")
                else:
                    # CRITICAL: Check actual test pass rate
                    test_success = test_execution_result.get("success", False) if test_execution_result else False
                    pass_rate = test_execution_result.get("pass_rate", 0.0) if test_execution_result else 0.0

                    if test_execution_result:
                        logger.info("ðŸ” TechLeadSWEA: Reviewing TestSWEA results - success=%s, pass_rate=%.1f%%", 
                                    test_success, pass_rate)

                        if not test_success or pass_rate < 100.0:
                            meets_standards = False
                            issues.append(f"Tests failed - only {pass_rate:.1f}% pass rate (100% required)")
                            stderr = test_execution_result.get("stderr", "")
                            if stderr:
                                issues.append(f"Test errors: {stderr[:200]}...")
                            logger.warning("âŒ TechLeadSWEA: REJECTING TestSWEA task - tests failed (%.1f%% pass rate)", pass_rate)
                        else:
                            # Tests passed - ensure some code exists
                            if not data.get("code") and not data.get("test_files"):
                                meets_standards = False
                                issues.append("Missing test code generation")
                            else:
                                logger.info("âœ… TechLeadSWEA: APPROVING TestSWEA task - all tests passed (100%% pass rate)")
                                logger.info("âœ… TechLeadSWEA: TestSWEA meets standards")

        return {
            "meets_standards": meets_standards,
            "issues": issues,
            "quality_level": "high" if meets_standards else "low",
        }

    def _check_technical_compliance(
        self, swea_agent: str, task_type: str, result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Check technical standards compliance"""
        compliant = True
        violations = []

        # Check for basic compliance indicators
        data = result.get("data", {})

        if "Backend" in swea_agent:
            # Check for proper code structure - Updated to use actual SWEA response format
            if "generate_model" in task_type:
                # BackendSWEA returns "code" not "model_code"
                model_code = data.get("code", "")
                if "from pydantic import BaseModel" not in model_code:
                    compliant = False
                    violations.append("Missing Pydantic BaseModel import")

            elif "generate_api" in task_type:
                # BackendSWEA returns "code" not "api_code" 
                api_code = data.get("code", "")
                if "from fastapi import" not in api_code:
                    compliant = False
                    violations.append("Missing FastAPI imports")

        elif "Test" in swea_agent:
            # CRITICAL: TestSWEA compliance - tests must pass to be compliant
            if "generate" in task_type:
                test_execution_result = result.get("test_execution_result")
                
                if not test_execution_result:
                    compliant = False
                    violations.append("Test execution results missing - compliance cannot be verified")
                else:
                    test_success = test_execution_result.get("success", False)
                    pass_rate = test_execution_result.get("pass_rate", 0.0)
                    
                    if not test_success or pass_rate < 100.0:
                        compliant = False
                        violations.append(f"Test compliance failed - {pass_rate:.1f}% pass rate (100% required)")

        return {
            "compliant": compliant,
            "violations": violations,
            "compliance_level": "high" if compliant else "low",
        }

    def _validate_business_alignment(self, entity: str, result: Dict[str, Any], swea_agent: str = "") -> Dict[str, Any]:
        """Validate business alignment of SWEA output"""
        try:
            # Check if result contains expected business vocabulary
            result_content = str(result).lower()
            entity_lower = entity.lower()
            
            # Use provided swea_agent or try to detect from task type
            if not swea_agent:
                task_type = result.get("task", "")
                if "database" in task_type.lower():
                    swea_agent = "Database"
                elif "backend" in task_type.lower() or "model" in task_type.lower() or "api" in task_type.lower():
                    swea_agent = "Backend"
                elif "frontend" in task_type.lower() or "ui" in task_type.lower():
                    swea_agent = "Frontend"

            misalignments = []
            
            if "Database" in swea_agent:
                # DatabaseSWEA business alignment - focus on data persistence and structure
                has_entity_reference = entity_lower in result_content
                has_database_terms = any(term in result_content for term in ["table", "database", "schema", "column"])
                
                if not has_entity_reference:
                    misalignments.append(f"Missing {entity} entity reference in database setup")
                if not has_database_terms:
                    misalignments.append("Missing database-specific vocabulary")
                
                semantic_coherence = has_entity_reference and has_database_terms
                
            elif "Frontend" in swea_agent:
                # Frontend/UI code - more lenient validation focusing on UI functionality
                has_ui_functionality = any(term in result_content for term in [
                    "title", "form", "button", "input", "display", "show", "interface", "ui"
                ])
                
                # For UI code, we don't strictly require entity names or CRUD terms
                # Just check that it has basic UI functionality
                if not has_ui_functionality:
                    misalignments.append("Missing basic UI functionality indicators")
                
                # UI code is semantically coherent if it has UI elements
                semantic_coherence = has_ui_functionality
                
            elif "Test" in swea_agent:
                # TestSWEA business alignment - focus on test execution success and entity coverage
                test_execution_result = result.get("test_execution_result")
                
                if not test_execution_result:
                    misalignments.append("Missing test execution results - cannot validate business alignment")
                    semantic_coherence = False
                else:
                    test_success = test_execution_result.get("success", False)
                    pass_rate = test_execution_result.get("pass_rate", 0.0)
                    
                    # Business alignment for tests means they validate the entity properly
                    if not test_success or pass_rate < 100.0:
                        misalignments.append(f"Tests do not validate {entity} entity properly - {pass_rate:.1f}% pass rate")
                        semantic_coherence = False
                    else:
                        # Tests passed - check for entity-specific test coverage
                        has_entity_tests = entity_lower in result_content
                        if not has_entity_tests:
                            misalignments.append(f"Missing {entity} entity-specific test coverage")
                        
                        semantic_coherence = test_success and has_entity_tests
                
            else:
                # Backend/API code - original validation logic for models and APIs
                has_entity_reference = entity_lower in result_content
                follows_naming_convention = entity_lower in result_content and (
                    "model" in result_content or "class" in result_content
                )

                # Domain-specific vocabulary preservation
                business_terms = ["create", "read", "update", "delete", "list"]
                has_business_terms = any(term in result_content for term in business_terms)

                # Semantic coherence check
                semantic_coherence = has_entity_reference and has_business_terms

                if not has_entity_reference:
                    misalignments.append(f"Missing {entity} entity reference")
                if not follows_naming_convention:
                    misalignments.append("Naming convention not followed")
                if not has_business_terms:
                    misalignments.append("Missing business vocabulary terms")

            return {
                "aligned": len(misalignments) == 0,
                "alignment_score": 1.0 - (len(misalignments) / max(1, len(misalignments))) if misalignments else 1.0,
                "misalignments": misalignments,
                "semantic_coherence": semantic_coherence,
            }

        except Exception as e:
            return {
                "aligned": False,
                "alignment_score": 0.0,
                "misalignments": [f"Business alignment validation failed: {str(e)}"],
                "semantic_coherence": False,
            }

    def _calculate_quality_score(
        self,
        quality_assessment: Dict[str, Any],
        compliance_check: Dict[str, Any],
        business_alignment: Dict[str, Any],
    ) -> float:
        """Calculate overall quality score"""
        quality_weight = 0.4
        compliance_weight = 0.3
        alignment_weight = 0.3

        quality_score = 1.0 if quality_assessment.get("meets_standards", False) else 0.0
        compliance_score = 1.0 if compliance_check.get("compliant", False) else 0.0
        alignment_score = business_alignment.get("alignment_score", 0.0)

        overall_score = (
            quality_score * quality_weight
            + compliance_score * compliance_weight
            + alignment_score * alignment_weight
        )

        return round(overall_score, 2)

    def _conduct_final_system_review(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """Conduct final system review with comprehensive technical governance"""
        try:
            entity = payload.get("entity", "Unknown")
            execution_results = payload.get("execution_results", [])

            logger.info("ðŸ§  TechLeadSWEA: Conducting final system review for %s", entity)

            # Analyze all execution results
            overall_success = True
            component_reviews = []

            for result in execution_results:
                if result.get("success", False):
                    component_reviews.append(
                        {
                            "component": result.get("task", "unknown"),
                            "status": "approved",
                            "quality_score": result.get("quality_score", 0.8),
                        }
                    )
                else:
                    overall_success = False
                    component_reviews.append(
                        {
                            "component": result.get("task", "unknown"),
                            "status": "rejected",
                            "issues": [result.get("error", "Unknown error")],
                        }
                    )

            # System integration assessment
            integration_score = self._assess_system_integration(execution_results)

            # Deployment readiness check
            deployment_ready = overall_success and integration_score > 0.7

            final_recommendations = []
            if deployment_ready:
                final_recommendations.extend([
                    "System ready for production deployment",
                    "Monitor system performance metrics",
                    "Implement continuous integration",
                ])
            else:
                final_recommendations.extend([
                    "Address component failures before deployment",
                    "Improve system integration score",
                    "Review technical architecture decisions",
                ])

            # Calculate system quality score
            quality_score = integration_score if overall_success else integration_score * 0.5

            # Simple decision log
            successful_components = len([r for r in execution_results if r.get("success", False)])
            self._log_decision("final_review", entity, "PASS" if overall_success else "FAIL",
                             f"quality score {quality_score:.2f}",
                             components_reviewed=len(execution_results),
                             successful_components=successful_components,
                             deployment_ready="YES" if deployment_ready else "NO")

            return {
                "success": True,
                "data": {
                    "overall_approval": overall_success,
                    "deployment_ready": deployment_ready,
                    "system_quality_score": quality_score,
                    "integration_score": integration_score,
                    "component_reviews": component_reviews,
                    "final_recommendations": final_recommendations,
                    "entity": entity,
                    "reviewed_components": len(execution_results),
                },
                "message": f"Final system review completed for {entity}",
                "technical_governance": True,
            }

        except Exception as e:
            logger.error("âŒ TechLeadSWEA final review failed: %s", str(e))
            return {
                "success": False,
                "error": f"Final system review failed: {str(e)}",
                "overall_approval": False,
                "technical_governance": False,
            }

    def _assess_system_integration(self, execution_results: List[Dict[str, Any]]) -> float:
        """Assess overall system integration quality"""
        if not execution_results:
            return 0.0

        successful_components = len([r for r in execution_results if r.get("success", False)])
        total_components = len(execution_results)

        integration_score = successful_components / total_components

        # Bonus for having all core components
        core_components = ["DatabaseSWEA", "BackendSWEA", "FrontendSWEA"]
        present_core = [
            c for c in core_components if any(c in r.get("task", "") for r in execution_results)
        ]

        if len(present_core) == len(core_components):
            integration_score += 0.1  # 10% bonus for complete system

        return min(integration_score, 1.0)

    def _get_timestamp(self) -> str:
        """Get current timestamp"""
        from datetime import datetime

        return datetime.now().strftime("%Y%m%d_%H%M%S")

    def _hybrid_coordination(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """
        Hybrid approach to analyze test failures and coordinate fixes:
        1. Error Pattern Analysis - Parse common error types
        2. Code Quality Analysis - Validate syntax and structure  
        3. LLM-Based Reasoning - AI analysis of complex scenarios
        4. Decision Matrix - Route fixes to appropriate SWEA agents
        """
        try:
            entity = payload.get("entity", "Unknown")
            execution_type = payload.get("execution_type", "creation_validation")
            failure_context = payload.get("failure_context", {})
            generated_artifacts = payload.get("generated_artifacts", [])
            coordination_id = payload.get("coordination_id", f"hybrid_{entity}")
            
            logger.info("ðŸ§  TechLeadSWEA: Starting hybrid analysis for %s (%s)", entity, execution_type)
            
            # For PoC: Check if system generation was successful
            generation_successful = self._check_generation_success(generated_artifacts)
            
            if generation_successful:
                # System generation was successful with STRICT validation (ALL tests passed)
                logger.info("âœ… TechLeadSWEA: System generation successful for %s - ALL TESTS PASSED", entity)
                
                # Simple decision log - success path
                self._log_decision("hybrid_coordination", entity, "APPROVED", 
                                 "strict validation passed",
                                 execution_type=execution_type,
                                 artifacts=len(generated_artifacts),
                                 fix_iterations=0)
                
                return self.create_success_response("hybrid_coordination", {
                    "success": True,
                    "final_success": True,
                    "fix_iterations": 0,
                    "coordination_log": [
                        "âœ… System generation completed successfully",
                        "ðŸŽ¯ STRICT VALIDATION: Complete system generated (database + models + API + UI + tests)",
                        "âœ… ALL TESTS PASSED: 100% test success rate achieved"
                    ],
                    "error_analysis": {"category": "strict_success", "confidence": 1.0},
                    "quality_analysis": {"total_artifacts": len(generated_artifacts), "issues_found": 0},
                    "llm_analysis": {"root_cause": "strict_success", "primary_swea": "N/A"},
                    "fix_decisions": [],
                    "coordination_id": coordination_id,
                    "strict_validation_passed": True,
                })
            
            # If generation failed, proceed with hybrid analysis
            max_retries = self._get_max_retries()
            fix_iterations = 0
            final_success = False
            coordination_log = []
            
            # Phase 1: Error Pattern Analysis
            error_analysis = self._analyze_error_patterns(failure_context)
            coordination_log.append(f"ðŸ” Error pattern: {error_analysis['category']} (confidence: {error_analysis['confidence']})")
            
            # Phase 2: Code Quality Analysis
            quality_analysis = self._analyze_code_quality(generated_artifacts, entity)
            coordination_log.append(f"ðŸ“Š Quality analysis: {quality_analysis['issues_found']} issues found")
            
            # Phase 3: LLM-Based Reasoning
            llm_analysis = self._perform_llm_analysis(failure_context, error_analysis, quality_analysis, entity)
            coordination_log.append(f"ðŸ§  LLM analysis: {llm_analysis['root_cause']} â†’ {llm_analysis['primary_swea']}")
            
            # Phase 4: Create Fix Decisions
            fix_decisions = self._create_fix_decisions(error_analysis, quality_analysis, llm_analysis, entity)
            coordination_log.append(f"ðŸŽ¯ Created {len(fix_decisions)} fix decisions")
            
            # Simple decision log - analysis phase
            self._log_decision("hybrid_coordination", entity, "ANALYZING", 
                             f"{len(fix_decisions)} fixes planned",
                             execution_type=execution_type,
                             error_category=error_analysis['category'],
                             quality_issues=quality_analysis['issues_found'],
                             primary_swea=llm_analysis['primary_swea'])
            
            # Phase 5: Iterative Fix Process (simplified for PoC)
            while fix_iterations < max_retries and not final_success:
                fix_iterations += 1
                coordination_log.append(f"ðŸ”„ Fix iteration {fix_iterations}/{max_retries}")
                
                # Execute fixes (simulation for PoC)
                fixes_applied = self._execute_coordinated_fixes(fix_decisions, entity, generated_artifacts)
                coordination_log.extend(fixes_applied)
                
                # Re-validate after fixes
                validation_result = self._validate_after_fixes(entity, execution_type)
                
                if validation_result.get("success"):
                    final_success = True
                    coordination_log.append("âœ… All tests passed after hybrid coordination")
                    break
                else:
                    # STRICT VALIDATION: System must pass ALL tests to be considered successful
                    # No leniency - if tests fail, the system is not ready
                    coordination_log.append("âŒ Tests still failing after fixes - system not ready")
                    
                    # Analyze remaining issues for next iteration
                    remaining_issues = validation_result.get("remaining_issues", [])
                    if remaining_issues:
                        coordination_log.append(f"âš ï¸ {len(remaining_issues)} issues remain, continuing...")
                        # Update fix decisions based on remaining issues
                        fix_decisions = self._refine_fix_decisions(fix_decisions, remaining_issues)
                    else:
                        coordination_log.append("âŒ No progress made, stopping iterations")
                        break
            
            # Final result
            result = {
                "success": final_success,
                "final_success": final_success,
                "fix_iterations": fix_iterations,
                "coordination_log": coordination_log,
                "error_analysis": error_analysis,
                "quality_analysis": quality_analysis,
                "llm_analysis": llm_analysis,
                "fix_decisions": fix_decisions,
                "coordination_id": coordination_id,
            }
            
            # ðŸ“Š DECISION SUMMARY LOG - FINAL RESULT
            logger.info("ðŸ“Š TechLeadSWEA HYBRID COORDINATION FINAL DECISION:")
            logger.info("   ðŸ“‹ Final Status: %s", "SUCCESS" if final_success else "FAILED")
            logger.info("   ðŸ”„ Iterations Used: %d/%d", fix_iterations, max_retries)
            logger.info("   ðŸ”§ Total Fixes Applied: %d", len(fix_decisions))
            logger.info("   ðŸ“Š Coordination Log: %d events", len(coordination_log))
            if final_success:
                logger.info("   âœ… System Status: APPROVED after %d fix iterations", fix_iterations)
            else:
                logger.info("   âŒ System Status: REJECTED - maximum iterations reached")
                result["error"] = f"Max fix iterations ({max_retries}) reached without success"
            
            if final_success:
                logger.info("âœ… TechLeadSWEA hybrid coordination successful for %s after %d iterations", 
                           entity, fix_iterations)
            else:
                logger.warning("âš ï¸ TechLeadSWEA hybrid coordination reached max iterations (%d) for %s", 
                              max_retries, entity)
            
            return self.create_success_response("hybrid_coordination", result)
            
        except Exception as e:
            logger.error("âŒ TechLeadSWEA hybrid coordination failed: %s", str(e))
            return self.create_error_response("hybrid_coordination", str(e), "hybrid_coordination_error")

    def _check_generation_success(self, generated_artifacts: List[Dict[str, Any]]) -> bool:
        """Check if system generation was successful - FIXED: Remove circular TestSWEA dependency"""
        if not generated_artifacts:
            return False
        
        # 1. Check for key artifacts that indicate successful generation
        # FIXED: Remove TestSWEA from required success criteria during fix coordination
        # TestSWEA success should be determined by actual test execution, not task completion
        required_sweas = ["DatabaseSWEA", "BackendSWEA", "FrontendSWEA"]
        successful_sweas = set()
        
        for artifact in generated_artifacts:
            task = artifact.get("task", "")
            success = artifact.get("success", False)
            
            if success:
                for swea in required_sweas:
                    if swea in task:
                        successful_sweas.add(swea)
        
        # 2. Core SWEAs must succeed (Database, Backend, Frontend)
        if len(successful_sweas) != len(required_sweas):
            logger.warning("âŒ Not all core SWEAs succeeded. Required: %s, Successful: %s", 
                          required_sweas, list(successful_sweas))
            return False
        
        # 3. FIXED: Do NOT check test pass rates for generation success
        # Generation success = Core SWEAs succeeded (Database, Backend, Frontend)
        # Test success is handled separately in the hybrid coordination flow
        # This breaks the circular dependency where generation success depends on test success
        
        # Just log test status for information, but don't use it for generation success determination
        latest_test_result = self._get_latest_test_execution_results(generated_artifacts)
        if latest_test_result:
            tests_passed = latest_test_result.get("tests_passed", 0)
            tests_executed = latest_test_result.get("tests_executed", 0)
            pass_rate = latest_test_result.get("pass_rate", 0.0)
            
            logger.info("ðŸ“Š Test status: %d/%d passed (%.1f%% pass rate)", tests_passed, tests_executed, pass_rate)
        else:
            # Check if tests were at least generated
            test_generated = any("TestSWEA" in artifact.get("task", "") and artifact.get("success", False) 
                               for artifact in generated_artifacts)
            if test_generated:
                logger.info("âœ… Tests generated successfully")
            else:
                logger.info("â„¹ï¸  No test generation detected")
        
        logger.info("âœ… System generation successful - core artifacts ready and tests passing")
        return True

    def _get_latest_test_execution_results(self, generated_artifacts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Extract the most recent test execution results from artifacts or runtime kernel"""
        # First, try to get from generated artifacts
        for artifact in generated_artifacts:
            task = artifact.get("task", "")
            if "TestSWEA" in task and artifact.get("success", False):
                result = artifact.get("result", {})
                if isinstance(result, dict):
                    # Check for test_execution_result (from runtime kernel)
                    if "test_execution_result" in result:
                        return result["test_execution_result"]
                    
                    data = result.get("data", {})
                    # Check for test_executions (plural) in the data
                    if "test_executions" in data:
                        test_executions = data["test_executions"]
                        # Return the last execution (most recent)
                        if test_executions and len(test_executions) > 0:
                            return test_executions[-1]
                    
                    # Fallback: check for single test_execution
                    elif "test_execution" in data:
                        return data["test_execution"]
        
        return {}

    def _get_test_execution_results(self, generated_artifacts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Extract test execution results from generated artifacts - DEPRECATED: Use _get_latest_test_execution_results"""
        return self._get_latest_test_execution_results(generated_artifacts)

    def _get_max_retries(self) -> int:
        """Get maximum retry attempts from environment configuration (used for fix iterations)"""
        import os
        return int(os.getenv("BAE_MAX_RETRIES", "3"))

    def _analyze_error_patterns(self, failure_context: Dict[str, Any]) -> Dict[str, Any]:
        """Phase 1: Analyze error patterns from test execution"""
        # Extract stderr and stdout from either direct or nested structure
        test_execution = failure_context.get("test_execution", {})
        stderr = (failure_context.get("stderr", "") or test_execution.get("stderr", "")).lower()
        stdout = (failure_context.get("stdout", "") or test_execution.get("stdout", "")).lower()
        exit_code = failure_context.get("exit_code", test_execution.get("exit_code", -1))
        
        # Analyze all possible error patterns and their confidence scores
        error_patterns = []
        
        # Check for syntax errors (highest priority)
        if "syntaxerror" in stderr or "invalid syntax" in stderr:
            error_patterns.append({
                "category": "syntax_error",
                "severity": "high",
                "likely_cause": "generated_code",
                "suggested_swea": "BackendSWEA",
                "confidence": 0.9
            })
        
        # Check for import errors
        if "modulenotfounderror" in stderr or "importerror" in stderr:
            error_patterns.append({
                "category": "import_error",
                "severity": "high",
                "likely_cause": "missing_dependencies",
                "suggested_swea": "BackendSWEA",
                "confidence": 0.8
            })
        
        # Check for API/endpoint errors
        if "404" in stderr or "not found" in stderr:
            error_patterns.append({
                "category": "endpoint_missing", 
                "severity": "high",
                "likely_cause": "api_routing",
                "suggested_swea": "BackendSWEA",
                "confidence": 0.8
            })
        
        # Check for assertion failures
        if "assertionerror" in stderr or "assertion" in stderr:
            error_patterns.append({
                "category": "assertion_failure",
                "severity": "medium", 
                "likely_cause": "test_logic",
                "suggested_swea": "TestSWEA",
                "confidence": 0.7
            })
        
        # Check for connection errors
        if "connectionerror" in stderr or "refused" in stderr:
            error_patterns.append({
                "category": "connection_error",
                "severity": "medium",
                "likely_cause": "server_not_running",
                "suggested_swea": "DatabaseSWEA",
                "confidence": 0.6
            })
        
        # If multiple patterns found, prioritize by severity and confidence
        if error_patterns:
            # Sort by severity (high first) then by confidence (highest first)
            severity_order = {"high": 3, "medium": 2, "low": 1}
            error_patterns.sort(
                key=lambda x: (severity_order.get(x["severity"], 0), x["confidence"]), 
                reverse=True
            )
            
            # Return the highest priority error pattern
            primary_error = error_patterns[0]
            
            # Add information about multiple errors if present
            if len(error_patterns) > 1:
                primary_error["multiple_errors"] = True
                primary_error["all_categories"] = [p["category"] for p in error_patterns]
                primary_error["secondary_patterns"] = error_patterns[1:]
            
            return primary_error
        
        # No specific patterns detected
        return {
            "category": "unknown_error",
            "severity": "medium",
            "likely_cause": "complex_issue",
            "suggested_swea": "TestSWEA",  # Default to test analysis
            "confidence": 0.3
        }

    def _analyze_code_quality(self, generated_artifacts: List[Dict[str, Any]], entity: str) -> Dict[str, Any]:
        """Phase 2: Analyze quality of generated code artifacts"""
        issues = []
        
        for artifact in generated_artifacts:
            task = artifact.get("task", "")
            success = artifact.get("success", False)
            result = artifact.get("result", {})
            
            if not success:
                issues.append({
                    "task": task,
                    "type": "generation_failure", 
                    "severity": "high",
                    "description": f"Failed to generate: {task}"
                })
                continue
            
            # Check for specific quality issues based on task type
            if "generate_model" in task:
                model_issues = self._check_model_quality(result)
                issues.extend(model_issues)
            elif "generate_api" in task:
                api_issues = self._check_api_quality(result)
                issues.extend(api_issues)
            elif "generate_ui" in task:
                ui_issues = self._check_ui_quality(result)
                issues.extend(ui_issues)
        
        return {
            "total_artifacts": len(generated_artifacts),
            "issues_found": len(issues),
            "issues": issues,
            "quality_score": max(0, 1.0 - (len(issues) / max(len(generated_artifacts), 1)))
        }

    def _check_model_quality(self, result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Check quality of generated model code"""
        issues = []
        data = result.get("data", {})
        code = data.get("code", "")
        
        # Basic syntax check
        if not code or len(code.strip()) < 10:
            issues.append({
                "type": "empty_model",
                "severity": "high", 
                "description": "Generated model is empty or too short"
            })
        
        # Check for required elements
        if "class " not in code:
            issues.append({
                "type": "missing_class",
                "severity": "high",
                "description": "No class definition found in model"
            })
        
        if "BaseModel" not in code and "pydantic" not in code.lower():
            issues.append({
                "type": "missing_basemodel",
                "severity": "medium",
                "description": "Model doesn't inherit from BaseModel"
            })
        
        return issues

    def _check_api_quality(self, result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Check quality of generated API code"""
        issues = []
        data = result.get("data", {})
        code = data.get("code", "")
        
        if not code or len(code.strip()) < 20:
            issues.append({
                "type": "empty_api",
                "severity": "high",
                "description": "Generated API is empty or too short"
            })
        
        # Check for required API elements
        required_elements = ["@router.", "def ", "fastapi"]
        for element in required_elements:
            if element not in code.lower():
                issues.append({
                    "type": f"missing_{element.replace('@', '').replace('.', '')}",
                    "severity": "medium",
                    "description": f"API missing required element: {element}"
                })
        
        return issues

    def _check_ui_quality(self, result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Check quality of generated UI code"""
        issues = []
        data = result.get("data", {})
        code = data.get("code", "")
        
        if not code or len(code.strip()) < 20:
            issues.append({
                "type": "empty_ui",
                "severity": "high",
                "description": "Generated UI is empty or too short"
            })
        
        # Check for Streamlit elements
        if "streamlit" not in code.lower() and "st." not in code:
            issues.append({
                "type": "missing_streamlit",
                "severity": "high",
                "description": "UI doesn't use Streamlit components"
            })
        
        return issues

    def _perform_llm_analysis(
        self, 
        failure_context: Dict[str, Any], 
        error_analysis: Dict[str, Any], 
        quality_analysis: Dict[str, Any],
        entity: str
    ) -> Dict[str, Any]:
        """Phase 3: LLM-based reasoning for complex failure analysis"""
        
        system_prompt = f"""You are a senior technical lead analyzing test failures in a generated system.

CONTEXT:
- Entity: {entity}
- Error Pattern: {error_analysis['category']} (confidence: {error_analysis['confidence']})
- Quality Issues: {quality_analysis['issues_found']} found
- Test Results: {failure_context.get('tests_executed', 0)} executed, {failure_context.get('tests_failed', 0)} failed

TASK:
Analyze the failure and determine:
1. Root cause category
2. Which SWEA agent should handle the fix
3. Specific fix actions needed
4. Confidence level in your analysis

RESPONSE FORMAT (JSON only):
{{
    "root_cause": "specific root cause description",
    "primary_swea": "BackendSWEA|FrontendSWEA|TestSWEA|DatabaseSWEA",
    "secondary_swea": "optional secondary SWEA if multiple fixes needed",
    "fix_actions": ["specific", "actions", "to", "take"],
    "confidence": 0.0-1.0,
    "reasoning": "explanation of analysis"
}}"""

        user_prompt = f"""Analyze this test failure:

STDERR: {failure_context.get('stderr', 'N/A')[:500]}
STDOUT: {failure_context.get('stdout', 'N/A')[:300]}

Error Analysis: {error_analysis}
Quality Issues: {[issue['description'] for issue in quality_analysis['issues'][:3]]}

What's the root cause and how should it be fixed?"""

        try:
            response = self.llm_client.generate_response(user_prompt, system_prompt)
            import json
            analysis = json.loads(response)
            
            # Validate response structure
            required_fields = ["root_cause", "primary_swea", "fix_actions", "confidence"]
            for field in required_fields:
                if field not in analysis:
                    analysis[field] = "unknown" if field != "confidence" else 0.5
            
            logger.debug("ðŸ§  LLM Analysis: %s (confidence: %.2f)", 
                        analysis['root_cause'], analysis['confidence'])
            return analysis
            
        except Exception as e:
            logger.warning("âš ï¸ LLM analysis failed, using fallback: %s", str(e))
            return {
                "root_cause": "llm_analysis_failed",
                "primary_swea": error_analysis.get("suggested_swea", "TestSWEA"),
                "fix_actions": ["regenerate_failing_component"],
                "confidence": 0.3,
                "reasoning": f"LLM analysis failed: {str(e)}"
            }

    def _create_fix_decisions(
        self,
        error_analysis: Dict[str, Any],
        quality_analysis: Dict[str, Any], 
        llm_analysis: Dict[str, Any],
        entity: str
    ) -> List[Dict[str, Any]]:
        """Phase 4: Create fix decisions based on hybrid analysis"""
        
        decisions = []
        
        # Primary fix based on LLM analysis
        primary_swea = llm_analysis.get("primary_swea", "TestSWEA")
        fix_actions = llm_analysis.get("fix_actions", ["regenerate"])
        
        primary_decision = {
            "swea_agent": primary_swea,
            "fix_actions": fix_actions,
            "priority": "high",
            "reasoning": llm_analysis.get("reasoning", "LLM analysis"),
            "confidence": llm_analysis.get("confidence", 0.5)
        }
        decisions.append(primary_decision)
        
        # Secondary fix if suggested
        secondary_swea = llm_analysis.get("secondary_swea")
        if secondary_swea and secondary_swea != primary_swea:
            secondary_decision = {
                "swea_agent": secondary_swea,
                "fix_actions": ["validate_and_fix"],
                "priority": "medium", 
                "reasoning": "Secondary fix from LLM analysis",
                "confidence": llm_analysis.get("confidence", 0.5) * 0.7
            }
            decisions.append(secondary_decision)
        
        # Quality-based fixes
        for issue in quality_analysis.get("issues", []):
            if issue.get("severity") == "high":
                swea_for_issue = self._map_issue_to_swea(issue)
                if not any(d["swea_agent"] == swea_for_issue for d in decisions):
                    quality_decision = {
                        "swea_agent": swea_for_issue,
                        "fix_actions": [f"fix_{issue['type']}"],
                        "priority": "medium",
                        "reasoning": f"Quality issue: {issue['description']}",
                        "confidence": 0.7
                    }
                    decisions.append(quality_decision)
        
        logger.debug("ðŸŽ¯ Created %d fix decisions for %s", len(decisions), entity)
        return decisions

    def _map_issue_to_swea(self, issue: Dict[str, Any]) -> str:
        """Map quality issue to appropriate SWEA agent"""
        issue_type = issue.get("type", "")
        
        if "model" in issue_type or "api" in issue_type:
            return "BackendSWEA"
        elif "ui" in issue_type or "streamlit" in issue_type:
            return "FrontendSWEA"
        elif "database" in issue_type or "schema" in issue_type:
            return "DatabaseSWEA"
        else:
            return "TestSWEA"

    def _execute_coordinated_fixes(
        self, 
        fix_decisions: List[Dict[str, Any]], 
        entity: str, 
        generated_artifacts: List[Dict[str, Any]]
    ) -> List[str]:
        """Execute fixes based on coordinated decisions"""
        
        coordination_log = []
        
        for decision in fix_decisions:
            swea_agent = decision["swea_agent"]
            fix_actions = decision.get("fix_actions", [])
            priority = decision.get("priority", "medium")
            
            coordination_log.append(f"ðŸ”§ Executing {priority} priority fix via {swea_agent}")
            
            # Create fix payload
            fix_payload = {
                "entity": entity,
                "fix_actions": fix_actions,
                "fix_context": {
                    "issue_type": decision.get("reasoning", "unknown"),
                    "priority": priority,
                    "confidence": decision.get("confidence", 0.5)
                },
                "generated_artifacts": generated_artifacts
            }
            
            # Route to appropriate SWEA (this would be handled by RuntimeKernel)
            coordination_log.append(f"   â†’ Routing fix to {swea_agent}: {fix_actions}")
        
        return coordination_log

    def _validate_after_fixes(self, entity: str, execution_type: str) -> Dict[str, Any]:
        """Validate system after applying fixes by re-executing EXISTING tests (not generating new ones)"""
        try:
            # CRITICAL FIX: Only execute existing tests, don't regenerate them
            # The tests were already generated during the main coordination flow
            # We just need to re-execute them after fixes are applied
            
            # Import TestSWEA to re-execute existing tests
            from ..swea_agents.test_swea import TestSWEA
            
            test_swea = TestSWEA()
            
            # Create payload for test EXECUTION only (not generation)
            test_payload = {
                "entity": entity,
                "entity_name": entity,
                "execution_type": "fix_validation",
                "context": "post_fix_validation",
                "execute_only": True  # Flag to indicate test execution only, no generation
            }
            
            # Re-execute EXISTING tests using TestSWEA
            logger.info("ðŸ”„ Re-executing existing tests after fixes via TestSWEA...")
            test_result = test_swea.handle_task("execute_tests", test_payload)
            
            if test_result.get("success", False):
                # Extract test execution results from TestSWEA execute_tests response
                result_data = test_result.get("data", {})
                test_execution_result = result_data.get("test_execution", {})
                
                tests_passed = test_execution_result.get("tests_passed", 0)
                tests_executed = test_execution_result.get("tests_executed", 0)
                pass_rate = (tests_passed / tests_executed * 100) if tests_executed > 0 else 0.0
                
                success = pass_rate >= 100.0
                
                remaining_issues = []
                if not success:
                    # Analyze stderr for specific issues
                    stderr = test_execution_result.get("stderr", "").lower()
                    if "syntaxerror" in stderr:
                        remaining_issues.append({"type": "syntax_error", "description": "Syntax errors still present"})
                    if "modulenotfounderror" in stderr:
                        remaining_issues.append({"type": "import_error", "description": "Import errors still present"})
                    if "assertionerror" in stderr:
                        remaining_issues.append({"type": "assertion_error", "description": "Test assertions still failing"})
                    if not remaining_issues:
                        remaining_issues.append({"type": "test_failure", "description": f"Tests failing: {tests_passed}/{tests_executed} passed"})
                
                logger.info("ðŸ§ª Test re-execution results: %d/%d passed (%.1f%% pass rate)", 
                           tests_passed, tests_executed, pass_rate)
                
                return {
                    "success": success,
                    "remaining_issues": remaining_issues,
                    "validation_type": execution_type,
                    "tests_passed": tests_passed,
                    "tests_executed": tests_executed,
                    "pass_rate": pass_rate,
                    "test_execution_result": test_execution_result
                }
            else:
                # Test execution failed
                error = test_result.get("error", "Unknown test execution error")
                logger.warning("âŒ Test re-execution failed: %s", error)
                return {
                    "success": False,
                    "remaining_issues": [{"type": "test_execution_failure", "description": f"Test execution failed: {error}"}],
                    "validation_type": execution_type
                }
                
        except Exception as e:
            logger.warning("âš ï¸ Validation after fixes failed: %s", str(e))
            return {
                "success": False,
                "remaining_issues": [{"type": "validation_error", "description": f"Validation failed: {str(e)}"}],
                "validation_type": execution_type
            }

    def _refine_fix_decisions(
        self, 
        current_decisions: List[Dict[str, Any]], 
        remaining_issues: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """Refine fix decisions based on remaining issues"""
        # For now, return current decisions
        # This can be enhanced to create new decisions based on remaining issues
        return current_decisions

    def _manage_quality_gate(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        # Implementation of _manage_quality_gate method
        pass

    # Helper methods for architecture decisions
    def _extract_technical_requirements(self, business_requirements: Dict[str, Any]) -> Dict[str, Any]:
        """Extract technical requirements from business requirements"""
        return {
            "domain_focus": business_requirements.get("domain_focus", True),
            "semantic_coherence": business_requirements.get("semantic_coherence", True),
            "scalability": business_requirements.get("scalability", "standard"),
            "performance": business_requirements.get("performance", "standard"),
            "security": business_requirements.get("security", "standard"),
        }

    def _select_architecture_patterns(self, entity: str, technical_requirements: Dict[str, Any]) -> List[str]:
        """Select appropriate architecture patterns based on requirements"""
        patterns = ["domain_driven_design", "restful_api", "mvc_pattern"]
        
        if technical_requirements.get("domain_focus"):
            patterns.append("business_entity_focus")
        if technical_requirements.get("semantic_coherence"):
            patterns.append("semantic_consistency")
            
        return patterns

    def _recommend_technology_stack(self, entity: str, technical_requirements: Dict[str, Any]) -> Dict[str, Any]:
        """Recommend technology stack based on requirements"""
        return {
            "primary_tech": "python_fastapi_streamlit",
            "database": "sqlite",
            "api_framework": "fastapi",
            "ui_framework": "streamlit",
            "validation": "pydantic",
            "testing": "pytest",
        }

    def _define_performance_specifications(self, technical_requirements: Dict[str, Any]) -> Dict[str, Any]:
        """Define performance specifications based on requirements"""
        return {
            "target_level": technical_requirements.get("performance", "standard"),
            "api_response_time": "< 200ms",
            "ui_load_time": "< 2s",
            "database_query_time": "< 100ms",
        }

    def _define_security_specifications(self, technical_requirements: Dict[str, Any]) -> Dict[str, Any]:
        """Define security specifications based on requirements"""
        return {
            "security_level": technical_requirements.get("security", "standard"),
            "input_validation": True,
            "sql_injection_prevention": True,
            "xss_protection": True,
            "authentication": "optional",
        }

    # Helper methods for conflict resolution
    def _determine_conflict_resolution_strategy(self, conflict_type: str, involved_sweas: List[str], conflict_details: Dict[str, Any]) -> str:
        """Determine the strategy for resolving technical conflicts"""
        if conflict_type == "resource_conflict":
            return "priority_based_allocation"
        elif conflict_type == "technical_disagreement":
            return "technical_authority_decision"
        elif conflict_type == "dependency_conflict":
            return "dependency_resolution_matrix"
        else:
            return "standard_governance_process"

    def _assign_swea_priorities(self, involved_sweas: List[str], conflict_details: Dict[str, Any]) -> Dict[str, int]:
        """Assign priorities to SWEAs involved in conflict"""
        priorities = {}
        for i, swea in enumerate(involved_sweas):
            priorities[swea] = i + 1  # Simple sequential priority
        return priorities

    def _define_technical_constraints(self, conflict_details: Dict[str, Any]) -> List[str]:
        """Define technical constraints for conflict resolution"""
        return [
            "maintain_system_integrity",
            "preserve_business_logic", 
            "ensure_backward_compatibility",
            "follow_coding_standards",
        ]

    def _log_decision(self, decision_type: str, entity: str, decision: str, rationale: str, **kwargs):
        """
        Centralized decision logging for all TechLeadSWEA decisions.
        Provides simple, concise summaries without code duplication.
        
        Args:
            decision_type: Type of decision (Architecture, Coordination, Conflict, etc.)
            entity: Entity being processed
            decision: The decision made (APPROVED, REJECTED, RESOLVED, etc.)
            rationale: Brief explanation of why this decision was made
            **kwargs: Additional context-specific information
        """
        # Only log technical details in debug mode
        if is_debug_mode():
            logger.info("ðŸ§  TechLeadSWEA %s: %s â†’ %s (%s)", 
                       decision_type.upper(), entity, decision, rationale)
            
            # Log additional context if provided
            for key, value in kwargs.items():
                if value is not None:
                    logger.info("   ðŸ“‹ %s: %s", key.replace('_', ' ').title(), value)
